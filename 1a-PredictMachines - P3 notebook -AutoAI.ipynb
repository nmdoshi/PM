{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "################################################################################\n#\n# Licensed Materials - Property of IBM\n# (C) Copyright IBM Corp. 2019\n# US Government Users Restricted Rights - Use, duplication disclosure restricted\n# by GSA ADP Schedule Contract with IBM Corp.\n#\n################################################################################\n"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# determine if autoai_libs needs to be updated in the service pod, and if so, install the minimum required version\nimport subprocess\ntarget_package = 'autoai-libs=='\nprocess = subprocess.run('pip freeze | grep ' + target_package, stdout=subprocess.PIPE, shell=True)\noutput = process.stdout.decode('ascii')\ninstalled_vrm = ['0','0','0']\nif output.startswith(target_package):\n    version_string = output[len(target_package):-1]  # remove newline from end too\n    installed_vrm = version_string.split('.')\nminimum_vrm = ['1','10','3']\nupdate_libs = False\nfor i, val in enumerate(installed_vrm):\n    if int(val) < int(minimum_vrm[i]):\n        update_libs = True\n        break\nif update_libs:\n    module_name = '/opt/ibm/.autoAI/autoai_libs-1.10.3-1-cp36-cp36m-linux_x86_64.whl'\n    print('attempting pip install of ' + module_name)\n    process = subprocess.Popen('pip install ' + module_name, shell=True)\n    process.wait()\n    print('installation successful')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## IBM AutoAI Auto-Generated Notebook v1.10.5\n### Representing Pipeline: P3 from run 1dfb0a2e-6c0e-4c4b-9ee0-9f73a7389ad2\n\n**Note**: Notebook code generated using AutoAI will execute successfully.  If code is modified or reordered, there is no guarantee it will successfully execute, please read our documentation for more information https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html .\n\nBefore modifying the pipeline or trying to re-fit the pipeline, consider:\nThe notebook converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\nThe known_values_list is passed by reference and populated with categorical values during fit of the preprocessing pipeline.  Delete its members before re-fitting.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1. Set Up"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "lightgbm, if needed, will be installed and imported later\n"
                }
            ],
            "source": "import sklearn\ntry:\n    import xgboost\nexcept:\n    print('xgboost, if needed, will be installed and imported later')\ntry:\n    import lightgbm\nexcept:\n    print('lightgbm, if needed, will be installed and imported later')\nfrom sklearn.cluster import FeatureAgglomeration\nimport numpy\nfrom numpy import nan, dtype, mean\nimport autoai_libs\nfrom autoai_libs.sklearn.custom_scorers import CustomScorers\nimport sklearn.ensemble\nfrom autoai_libs.cognito.transforms.transform_utils import TExtras, FC\nfrom autoai_libs.transformers.exportable import *\nfrom autoai_libs.utils.exportable_utils import *\nfrom sklearn.pipeline import Pipeline\nknown_values_list=[]\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2. Compose Pipeline"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "re-attempting import of transform_extras from autoai_libs.cognito.transforms\nPipeline successfully instantiated\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# Metadata used in retrieving data and computing metrics.  Customize as necessary for your environment.\ndata_source='replace_with_path_and_csv_filename'\ntarget_label_name = _input_metadata['target_label_name']\nlearning_type = _input_metadata['learning_type']\noptimization_metric = _input_metadata['optimization_metric']\nrandom_state = _input_metadata['random_state']\ncv_num_folds = _input_metadata['cv_num_folds']\nholdout_fraction = _input_metadata['holdout_fraction']\ndata_provenance = _input_metadata['data_provenance']\n"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "data_location {'id': '1', 'type': 's3', 'connection': {'access_key_id': '2ed76c071b6448ff9f603f30b3431b8a', 'secret_access_key': '4102030211f936020bf8de5391bfe21e2ca36d23265a6ba3', 'endpoint_url': 'https://s3-api.us-geo.objectstorage.softlayer.net'}, 'location': {'type': 's3', 'bucket': 'predictivemaintenance-donotdelete-pr-mlejlc5nq1ttru', 'path': 'historical.csv'}}\nlocal_path historical.csv\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning)\n"
                }
            ],
            "source": "import pandas as pd\ndf = None\nif type(data_provenance) is str:\n    try:\n        df = pd.read_csv(data_provenance) # your data file name here\n    except Exception as e:\n        print(e)\nif df is None:\n    try:\n        data_location = data_provenance['input_data'][0]\n        print('data_location '+ str(data_location))\n        import boto3\n        session = boto3.session.Session()\n        cos = session.client(\n            service_name='s3',\n            aws_access_key_id=data_location['connection']['access_key_id'],\n            aws_secret_access_key=data_location['connection']['secret_access_key'],\n            endpoint_url=data_location['connection']['endpoint_url'],\n            verify=False\n        )\n        local_path = data_location['location']['path']\n        print('local_path ' + str(local_path))\n        cos.download_file(data_location['location']['bucket'],\n                     data_location['location']['path'],\n                     local_path)\n        df = pd.read_csv(local_path) # your data file name here\n    except Exception as e:\n        print(e)\nif df is None:\n    raise(ValueError('need location or credential information to read dataframe from COS'))\n"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "target = target_label_name # your target name here\ndf_y = df[target]\n"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "df_X = df.drop(columns=[target])"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.model_selection import train_test_split"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# until the problem type is available in the metadata, use the sklearn type_of_target to determine whether to stratify the holdout split\nfrom sklearn.utils.multiclass import type_of_target\nif type_of_target(df_y.values) in ['multiclass', 'binary']:\n    X, X_holdout, y, y_holdout = train_test_split(df_X.values, df_y.values, test_size=holdout_fraction, random_state=random_state, stratify=df_y.values)\nelse:\n    X, X_holdout, y, y_holdout = train_test_split(df_X.values, df_y.values, test_size=holdout_fraction, random_state=random_state)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "Pipeline(memory=None,\n     steps=[('preprocessor', Pipeline(memory=None,\n     steps=[('features', FeatureUnion(n_jobs=None,\n       transformer_list=[('categorical', Pipeline(memory=None,\n     steps=[('cat_column_selector', NumpyColumnSelector(columns=[0])), ('cat_compress_strings', CompressStrings(activate_flag=True, compress...n'))])), ('estimator', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True))])"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "pipeline.fit(X, y)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "y_pred = pipeline.predict(X_holdout)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[ 8 12  7  4  4  7 13 13  7  4  9  3  8 14  3 10 11  8  4  9  9  8  8 10\n 10 10  8 11  9 14 11  4  5  7  6 12  6 12  8  3  6  7  7  5  7  6 10  9\n  8  8  8  4 15  9  7  6  9  6 13  3  9  3 10  4  5 10 11  5  5  5 12  5\n  7  8  3  4  6  5  5  5  6  9  4  9  6  6  5  7 11 13  9  7  5  6 11  8\n  5  6  7 14]\n[ 9.726476   9.077933  10.343016   4.7213955  5.569722   5.381709\n 10.571469  10.683764   7.489379   6.3740926  9.917762   4.939227\n  7.3512125 11.098298   4.507491   9.821881  10.360441   6.741496\n  6.2490616  6.823009  10.623696   8.668901   9.632336  11.748581\n 10.236459   7.788136   9.386002   8.973754   9.726275  11.255938\n  8.890341   4.530704   7.6558933  8.206708   5.060564   8.834135\n  7.776186  10.984617   9.570418   4.535057   7.529214   9.044755\n  6.064064   4.228711   9.162923   4.915103  11.594592   9.087476\n  8.188423   9.624701   7.7277     4.4880695 11.225461   7.680917\n  8.312495   5.3793387 11.272545   7.0352373 10.105771   4.457984\n  7.6342344  3.909327  10.562271   6.160308   7.465516   7.905446\n  8.181377   7.3767395  4.0364995  6.388747  11.399161   5.649658\n  5.387752  10.773125   4.523729   4.1422505  6.1042447  6.2623053\n  6.0936403  5.1882324  8.394014  10.081711   6.1015477  7.566188\n  6.3535223  7.4079847  5.444542   6.2390184  8.568908  11.322436\n  8.854716  10.507769   4.7276754  5.2184515  8.238973  10.409084\n  4.6065063  5.4284325  5.3888264 10.803    ]\n"
                }
            ],
            "source": "print(y_holdout)\nprint(y_pred)"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "-1.7428315383058257"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from sklearn.metrics import get_scorer\nscorer = get_scorer(optimization_metric)\nscorer(pipeline, X_holdout, y_holdout)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "#delete CONTENTS of known_values_list before refitting, cloning or cross_validate-ing the pipeline, or previous values will be used.\nfor i in range(len(known_values_list)):\n    del known_values_list[0]"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "# preprocessor should see all data for cross_validate on the remaining steps to match autoai scores\npreprocessor = pipeline.steps[0][1]\npreprocessor.fit(X)\nX_prep = preprocessor.transform(X)"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# remove preprocessor from pipeline\ndel pipeline.steps[0]"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n  warnings.warn(CV_WARNING, FutureWarning)\n"
                },
                {
                    "data": {
                        "text/plain": "-1.7323305093199988"
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# cross_validate on remaining steps of pipeline\nfrom sklearn.model_selection import cross_validate\ncv_results = cross_validate(pipeline, X_prep, y, scoring={optimization_metric:scorer})\nimport numpy as np\nnp.mean(cv_results['test_' + optimization_metric])\n"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_neg_root_mean_squared_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n  warnings.warn(*warn_args, **warn_kwargs)\n"
                },
                {
                    "data": {
                        "text/plain": "{'fit_time': array([2.08886576, 1.7591331 , 1.73914862]),\n 'score_time': array([0.15585518, 0.17067647, 0.20211554]),\n 'test_neg_root_mean_squared_error': array([-1.75098564, -1.6831234 , -1.76288249]),\n 'train_neg_root_mean_squared_error': array([-1.6643187 , -1.71820565, -1.66904996])}"
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "cv_results"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}